{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b812fd61",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_n           0\n",
       "id              1\n",
       "name            0\n",
       "gender          0\n",
       "species         0\n",
       "birthday        0\n",
       "personality     0\n",
       "song           11\n",
       "phrase          0\n",
       "full_id         0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad344ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a synthetic dataset with missing values to simulate the pandas missing values dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Creating a dictionary with some missing values\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', np.nan, 'David', 'Eva'],\n",
    "    'Age': [25, 30, np.nan, 22, 29],\n",
    "    'Score': [88, 92, 85, np.nan, 95],\n",
    "    'Country': ['USA', 'Canada', np.nan, 'USA', 'Germany']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Get the shape of the DataFrame (rows, columns)\n",
    "df_shape = df.shape\n",
    "df_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1e0e5",
   "metadata": {},
   "source": [
    "My own defition on \"observations\" and \"varaibles\": \"observations\", which is rows according\n",
    "to the dateset in statistics, is a collection of measurements obtained by an individual. \n",
    "\"variabels\", which is columns according to the dateset in statistics, is a feature of\n",
    "something or someone that differs from other individuals that is measured for each\n",
    "observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3c80f",
   "metadata": {},
   "source": [
    "df.shape: This gives the total number of rows and columns in the DataFrame. It reports the full structure of the dataset, regardless of the type of data in each column.\n",
    "df.describe(): By default, df.describe() only provides summary statistics for numerical columns (e.g., integers and floats). This means it excludes non-numerical columns like strings (object type) unless specified otherwise (e.g., df.describe(include='all')).\n",
    "\n",
    "df.shape[0]: This represents the total number of rows (observations) in the dataset, whether or not there are missing values in those rows.\n",
    "df.describe() \"count\": The \"count\" column in df.describe() reports the number of non-missing (non-null) values for each variable. It excludes rows with missing values in the corresponding column.\n",
    "\n",
    "\n",
    "Number of columns analyzed: df.describe() by default only includes numerical columns, while df.shape counts all columns.\n",
    "\"Count\" values in df.describe(): These represent the number of non-missing values in each numerical column, whereas df.shape counts the total number of rows, even if some contain missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91840192",
   "metadata": {},
   "source": [
    "Attributes:\n",
    "Represent static properties or values.\n",
    "Accessed without parentheses (df.shape).\n",
    "Do not change the DataFrame or perform any computation.\n",
    "Methods:\n",
    "Perform actions, calculations, or transformations.\n",
    "Accessed with parentheses (df.describe()).\n",
    "May take arguments and often return a modified DataFrame or computed result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd8685",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/66f1ee10-3744-8000-9eae-8fc66383b7be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55cc42",
   "metadata": {},
   "source": [
    "Count:\n",
    "The number of non-null (non-missing) entries for each variable/column.\n",
    "Mean:\n",
    "The arithmetic average of the data in the column. It's calculated as the sum of all values divided by the count (number of values).\n",
    "Std (Standard Deviation):\n",
    "A measure of the dispersion or spread of data from the mean. It indicates how much the data points typically deviate from the mean.\n",
    "Min (Minimum):\n",
    "The smallest value in the data for each variable/column.\n",
    "25% (1st Quartile):\n",
    "The 25th percentile, also known as the first quartile (Q1). It represents the value below which 25% of the data fall. It gives insight into the lower range of the data.\n",
    "50% (Median or 2nd Quartile):\n",
    "The median or 50th percentile, representing the middle value in the dataset when sorted. It divides the data into two equal halves and is less affected by outliers compared to the mean.\n",
    "75% (3rd Quartile):\n",
    "The 75th percentile, also known as the third quartile (Q3). It represents the value below which 75% of the data fall, providing insight into the upper range of the data.\n",
    "Max (Maximum):\n",
    "The largest value in the dataset for each variable/column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a42516",
   "metadata": {},
   "source": [
    "When missing data is scattered across rows, dropping rows might be a better choice.\n",
    "When missing data is concentrated within certain columns, dropping columns may be a more efficient option. \n",
    "In many cases, a combination of both methods may provide the most efficient use of non-missing data.\n",
    "First, use del df['col'] to remove columns that have a large proportion of missing data, reducing noise and potential bias.\n",
    "Then, use df.dropna() to remove any remaining rows with missing data, as the overall missingness would have reduced after removing the problematic columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f15eaa",
   "metadata": {},
   "source": [
    "Scenario: You have a dataset containing customer feedback with multiple features, including ratings, comments, and demographics.\n",
    "Use Case:\n",
    "Suppose the dataset has 1,000 entries, but only 10 rows have missing values in the Rating column.\n",
    "Since the Rating is crucial for your analysis (e.g., customer satisfaction), and only a small number of entries are missing, you would use df.dropna() to remove just those 10 rows.\n",
    "This preserves the integrity of the dataset, allowing you to retain a majority of the data for analysis without losing important columns.\n",
    "\n",
    "Scenario: You have a dataset of student grades with columns for Student ID, Name, Assignment Scores, and Extracurricular Activities.\n",
    "Use Case:\n",
    "Imagine that the Extracurricular Activities column has 80% missing values while the other columns are complete.\n",
    "In this case, using del df['Extracurricular Activities'] would be preferred.\n",
    "Since this column contributes little information due to the high percentage of missing data, deleting it allows you to keep all the rows intact and focus on the more reliable columns for your analysis.\n",
    "\n",
    "When using both methods, applying del df['col'] before df.dropna() can help in the following ways:\n",
    "Reduce Missingness: Removing columns with high missing values can decrease the overall missingness across the dataset, meaning that fewer rows will be dropped when df.dropna() is applied afterward.\n",
    "Maintain Row Integrity: If you have several columns with scattered missing values, dropping the ones with excessive missing data first may allow you to keep more rows that contain valuable information.\n",
    "Streamlined Analysis: It simplifies the dataset for analysis by eliminating unreliable data early on, making the subsequent row removal process more effective.\n",
    "    \n",
    "Justification for Approach\n",
    "By removing the Extracurricular Activities column first, which had a high percentage of missing values, I ensured that I retained the maximum number of rows with valid data in the Score column. After removing this column, I used df.dropna() to clean up any remaining missing values, allowing me to maintain a clean and effective dataset for further analysis. This strategy minimizes data loss while focusing on retaining the most relevant information.\n",
    "\n",
    "The remove action using some combination of del df['col'] and/or df.dropna() is as follows in the next In."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f765be7f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name  Score\n",
      "0   1    Alice   85.0\n",
      "2   3  Charlie   90.0\n",
      "3   4    David   78.0\n",
      "5   6    Frank   92.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n",
    "    'Score': [85, np.nan, 90, 78, np.nan, 92],\n",
    "    'Extracurricular Activities': ['Football', 'Basketball', np.nan, 'Soccer', np.nan, 'Chess']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Remove the column with high missingness\n",
    "del df['Extracurricular Activities']\n",
    "\n",
    "# Step 2: Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Final DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7dd36",
   "metadata": {},
   "source": [
    "df.groupby(\"col1\"): This part of the command groups the DataFrame by the unique values in col1. Each unique value in col1 forms a separate group.\n",
    "[\"col2\"]: This specifies that we are interested in the col2 column for our analysis.\n",
    ".describe(): This function generates descriptive statistics for col2 for each group created by col1. The output will typically include:\n",
    "count: Number of non-null entries in col2.\n",
    "mean: Average of col2 for the group.\n",
    "std: Standard deviation of col2.\n",
    "min: Minimum value of col2.\n",
    "25%: 25th percentile (1st quartile) of col2.\n",
    "50%: Median (2nd quartile) of col2.\n",
    "75%: 75th percentile (3rd quartile) of col2.\n",
    "max: Maximum value of col2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37475ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],\n",
    "    'Values': [10, 20, 30, 15, 25, 35, 5, 15, 25, None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd11474",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count  mean   std   min   25%   50%   75%   max\n",
      "Category                                                 \n",
      "A           3.0  20.0  10.0  10.0  15.0  20.0  25.0  30.0\n",
      "B           3.0  25.0  10.0  15.0  20.0  25.0  30.0  35.0\n",
      "C           3.0  15.0  10.0   5.0  10.0  15.0  20.0  25.0\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Category' and describe 'Values'\n",
    "result = df.groupby(\"Category\")[\"Values\"].describe()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c40c29",
   "metadata": {},
   "source": [
    "Count: The number of valid (non-null) entries for Values in each category:\n",
    "Category A: 3\n",
    "Category B: 3\n",
    "Category C: 3\n",
    "Mean: The average of Values for each category:\n",
    "Category A: (10 + 20 + 30) / 3 = 20.0\n",
    "Category B: (15 + 25 + 35) / 3 = 25.0\n",
    "Category C: (5 + 15 + 25) / 3 = 15.0\n",
    "Standard Deviation (std): Measures the spread of Values for each category:\n",
    "All categories have a standard deviation of 10.0, indicating how much the values vary around the mean.\n",
    "Min, 25%, 50%, 75%, Max: These values provide insights into the distribution of Values:\n",
    "Min: The smallest value in each category.\n",
    "25% (1st quartile): The value below which 25% of the data fall.\n",
    "50% (Median): The middle value of Values.\n",
    "75% (3rd quartile): The value below which 75% of the data fall.\n",
    "Max: The largest value in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55646fba",
   "metadata": {},
   "source": [
    "While df.describe() provides an overall view of the dataset, including how many entries are\n",
    "missing across all columns, df.groupby(\"col1\")[\"col2\"].describe() gives a detailed \n",
    "breakdown that highlights the distribution of valid data across specific categories. \n",
    "This distinction is crucial for understanding the context and reliability of the statistics,\n",
    "particularly when considering how missing data impacts analysis and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46312945",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# No import statement\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitanic.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
     ]
    }
   ],
   "source": [
    "# No import statement\n",
    "df = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3043b",
   "metadata": {},
   "source": [
    "NameError: name 'pd' is not defined\n",
    "\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'titanics.csv'\n",
    "\n",
    "NameError: name 'df' is not defined\n",
    "    \n",
    "SyntaxError: unexpected EOF while parsing\n",
    "    \n",
    "AttributeError: 'DataFrame' object has no attribute 'group_by'\n",
    "\n",
    "KeyError: 'Sex'\n",
    "\n",
    "NameError: name 'sex' is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9135c5",
   "metadata": {},
   "source": [
    "question 9:Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d920325",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/66f1fa7e-38d0-8000-bcc9-950d8fc187a4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
